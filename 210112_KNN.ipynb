# GitHub URL for Wine dataset
GITHUB_RAW_URL = "https://raw.githubusercontent.com/mitra369/KNN_210112/main/wine.csv"

# Drive path (if needed as backup)
DRIVE_PATH = "/content/drive/MyDrive/ML/wine.csv"
import pandas as pd
import os

df = None
if GITHUB_RAW_URL:
    try:
        df = pd.read_csv("https://raw.githubusercontent.com/mitra369/KNN_210112/main/wine.csv")
        print("Loaded Wine dataset from GitHub raw URL.")
    except Exception as e:
        print("Failed to load from GitHub URL:", e)

if df is None:
    from google.colab import drive
    drive.mount('/content/drive')
    if os.path.exists(DRIVE_PATH):
        df = pd.read_csv(DRIVE_PATH)
        print("Loaded Wine dataset from Google Drive.")
    else:
        raise FileNotFoundError(f"Could not find file at GitHub or Drive. Please verify GITHUB_RAW_URL or DRIVE_PATH: {DRIVE_PATH}")

print("Shape:", df.shape)
print("Expected shape: (178, 14) - 178 samples, 13 features + 1 target column")
df.head()
print("Columns:", df.columns.tolist())
print("\nDtypes:\n", df.dtypes)
print("\nNull counts:\n", df.isnull().sum())
print("\nValue counts for target column:")

# Wine dataset typically has target column named 'class', 'Wine', or 'target'
for c in ['class', 'Class', 'Wine', 'target', 'label', 'cultivar']:
    if c in df.columns:
        print(f"\n{c}:\n", df[c].value_counts(dropna=False).sort_index())

display(df.describe().T)

df.columns = [c.strip() for c in df.columns]


df = df.loc[:, ~df.columns.str.contains("^Unnamed")]

possible_targets = ['class', 'Class', 'Wine', 'target', 'label', 'cultivar', 'Type']
target_col = None
for t in possible_targets:
    if t in df.columns:
        target_col = t
        break

if target_col is None:
    raise ValueError("Could not find a target column automatically. Rename the target column to 'class' or 'target' and re-run.")

print("Detected target column:", target_col)


if df[target_col].dtype == object:
    df[target_col] = pd.to_numeric(df[target_col], errors='coerce')
    print("Converted target column to numeric")


print(f"Target classes: {sorted(df[target_col].unique())}")
print(f"Class distribution:\n{df[target_col].value_counts().sort_index()}")


for id_col in ['id', 'ID', 'Id', 'sample_id']:
    if id_col in df.columns:
        df = df.drop(columns=[id_col])
        print(f"Dropped column: {id_col}")

print("Final columns:", df.columns.tolist())
print(f"Final shape: {df.shape}")
from sklearn.model_selection import train_test_split


X = df.drop(columns=[target_col])
y = df[target_col]

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"Feature columns: {X.columns.tolist()}")


X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.30, random_state=42, stratify=y
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

print("\nData split complete:")
print(f"Train: {X_train.shape} ({len(X_train)/len(X)*100:.1f}%)")
print(f"Validation: {X_val.shape} ({len(X_val)/len(X)*100:.1f}%)")
print(f"Test: {X_test.shape} ({len(X_test)/len(X)*100:.1f}%)")

print(f"\nTrain class distribution:\n{y_train.value_counts().sort_index()}")
print(f"\nValidation class distribution:\n{y_val.value_counts().sort_index()}")
print(f"\nTest class distribution:\n{y_test.value_counts().sort_index()}")
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler


num_cols = X_train.select_dtypes(include=['int64','float64','int32','float32']).columns.tolist()
cat_cols = X_train.select_dtypes(include=['object','category','bool']).columns.tolist()

print("Numeric cols:", num_cols)
print(f"Number of numeric features: {len(num_cols)}")
print("Categorical cols:", cat_cols)
print(f"Number of categorical features: {len(cat_cols)}")


num_imputer = SimpleImputer(strategy='median')
scaler = StandardScaler()

X_train_num = X_train[num_cols].copy()
X_val_num   = X_val[num_cols].copy()
X_test_num  = X_test[num_cols].copy()


X_train_num_imp = pd.DataFrame(num_imputer.fit_transform(X_train_num), columns=num_cols, index=X_train_num.index)
X_val_num_imp   = pd.DataFrame(num_imputer.transform(X_val_num), columns=num_cols, index=X_val_num.index)
X_test_num_imp  = pd.DataFrame(num_imputer.transform(X_test_num), columns=num_cols, index=X_test_num.index)

print(f"\nMissing values after imputation - Train: {X_train_num_imp.isnull().sum().sum()}")

X_train_num_scaled = pd.DataFrame(scaler.fit_transform(X_train_num_imp), columns=num_cols, index=X_train_num_imp.index)
X_val_num_scaled   = pd.DataFrame(scaler.transform(X_val_num_imp), columns=num_cols, index=X_val_num_imp.index)
X_test_num_scaled  = pd.DataFrame(scaler.transform(X_test_num_imp), columns=num_cols, index=X_test_num_imp.index)

print("Scaling complete - Features now have mean=0, std=1")

if cat_cols:
    from sklearn.preprocessing import OneHotEncoder
    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False, drop='first')
    ohe.fit(X_train[cat_cols])
    X_train_cat = pd.DataFrame(ohe.transform(X_train[cat_cols]), index=X_train.index, columns=ohe.get_feature_names_out(cat_cols))
    X_val_cat   = pd.DataFrame(ohe.transform(X_val[cat_cols]), index=X_val.index, columns=ohe.get_feature_names_out(cat_cols))
    X_test_cat  = pd.DataFrame(ohe.transform(X_test[cat_cols]), index=X_test.index, columns=ohe.get_feature_names_out(cat_cols))

    X_train_proc = pd.concat([X_train_num_scaled, X_train_cat], axis=1)
    X_val_proc   = pd.concat([X_val_num_scaled, X_val_cat], axis=1)
    X_test_proc  = pd.concat([X_test_num_scaled, X_test_cat], axis=1)
else:
    X_train_proc = X_train_num_scaled
    X_val_proc   = X_val_num_scaled
    X_test_proc  = X_test_num_scaled

print(f"\nProcessed shapes:")
print(f"Train: {X_train_proc.shape}")
print(f"Validation: {X_val_proc.shape}")
print(f"Test: {X_test_proc.shape}")
print(f"\nAll 13 chemical features are now standardized and ready for KNN!")

display(X_train_proc.head())
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='minkowski')
knn.fit(X_train_proc, y_train)

y_val_pred = knn.predict(X_val_proc)
y_val_prob = knn.predict_proba(X_val_proc)  # All class probabilities for multi-class

print("Baseline KNN (k=5) on Validation Set:")
print("=" * 50)
print(f"Accuracy: {accuracy_score(y_val, y_val_pred):.4f}")

print(f"Precision (weighted): {precision_score(y_val, y_val_pred, average='weighted'):.4f}")
print(f"Recall (weighted): {recall_score(y_val, y_val_pred, average='weighted'):.4f}")
print(f"F1-Score (weighted): {f1_score(y_val, y_val_pred, average='weighted'):.4f}")

try:
    auc_ovr = roc_auc_score(y_val, y_val_prob, multi_class='ovr', average='weighted')
    print(f"AUC (weighted, OvR): {auc_ovr:.4f}")
except:
    print("AUC: Could not calculate (multi-class)")

print("\nPer-class metrics:")
print(f"Precision (macro): {precision_score(y_val, y_val_pred, average='macro'):.4f}")
print(f"Recall (macro): {recall_score(y_val, y_val_pred, average='macro'):.4f}")
print(f"F1-Score (macro): {f1_score(y_val, y_val_pred, average='macro'):.4f}")
import matplotlib.pyplot as plt
import numpy as np

k_values = list(range(1, 31))
val_scores = []

print("Testing k values from 1 to 30...")
for k in k_values:
    m = KNeighborsClassifier(n_neighbors=k, weights='uniform', metric='minkowski')
    m.fit(X_train_proc, y_train)
    val_scores.append(accuracy_score(y_val, m.predict(X_val_proc)))

plt.figure(figsize=(12, 6))
plt.plot(k_values, val_scores, marker='o', linewidth=2, markersize=6, color='blue')
plt.xticks(k_values)
plt.xlabel('k (Number of Neighbors)', fontsize=12)
plt.ylabel('Validation Accuracy', fontsize=12)
plt.title('Elbow Plot: Accuracy vs k-Value (Wine Dataset - 3 Classes)', fontsize=14, fontweight='bold')

best_k = k_values[int(pd.Series(val_scores).idxmax())]
best_acc = max(val_scores)
plt.axvline(best_k, color='red', linestyle='--', linewidth=2, label=f'Best k={best_k} (Acc={best_acc:.4f})')
plt.scatter([best_k], [best_acc], color='red', s=200, zorder=5, edgecolors='black', linewidth=2)

plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("=" * 60)
print(f"Best k on validation set: {best_k}")
print(f"Best validation accuracy: {best_acc:.4f}")
print(f"Accuracy range: {min(val_scores):.4f} to {max(val_scores):.4f}")
print("=" * 60)
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

pipe = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('knn', KNeighborsClassifier())
])

param_grid = {
    'knn__n_neighbors': [3, 5, 7, 9, 11, 13, 15],
    'knn__weights': ['uniform', 'distance'],
    'knn__metric': ['minkowski', 'euclidean', 'manhattan'],
    'knn__p': [1, 2]
}

print("Starting GridSearchCV...")
print(f"Total combinations to test: {len(param_grid['knn__n_neighbors']) * len(param_grid['knn__weights']) * len(param_grid['knn__metric']) * len(param_grid['knn__p'])}")
print("=" * 60)


grid = GridSearchCV(
    pipe,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2
)

grid.fit(X_train, y_train)

print("\n" + "=" * 60)
print("GridSearchCV Complete!")
print("=" * 60)
print("Best parameters found:")
for param, value in grid.best_params_.items():
    print(f"  {param}: {value}")
print(f"\nBest Cross-Validation Accuracy: {grid.best_score_:.4f}")
print("=" * 60)

best_grid = grid.best_estimator_
y_val_pred_best = best_grid.predict(X_val)
y_val_prob_best = best_grid.predict_proba(X_val)

from sklearn.metrics import classification_report

print("=" * 60)
print("VALIDATION SET METRICS (Best Model from GridSearch)")
print("=" * 60)
print(classification_report(y_val, y_val_pred_best, target_names=['Class 0', 'Class 1', 'Class 2']))


try:
    auc_val = roc_auc_score(y_val, y_val_prob_best, multi_class='ovr', average='weighted')
    print(f"AUC (weighted, OvR): {auc_val:.4f}")
except Exception as e:
    print(f"AUC calculation: {e}")

print("\n" + "=" * 60)
print("RETRAINING ON TRAIN + VALIDATION DATA")
print("=" * 60)

import numpy as np
X_trainval = pd.concat([X_train, X_val], axis=0)
y_trainval = pd.concat([y_train, y_val], axis=0)

print(f"Combined training data shape: {X_trainval.shape}")

best_params = grid.best_params_
knn_params = {k.split('__')[-1]: v for k, v in best_params.items() if k.startswith('knn__')}

print(f"Using best parameters: {knn_params}")

final_pipe = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('knn', KNeighborsClassifier(**knn_params))
])

final_pipe.fit(X_trainval, y_trainval)

y_test_pred = final_pipe.predict(X_test)
y_test_prob = final_pipe.predict_proba(X_test)  # All class probabilities

print("\n" + "=" * 60)
print("FINAL TEST SET METRICS (Train+Val → Test)")
print("=" * 60)
print(classification_report(y_test, y_test_pred, target_names=['Class 0', 'Class 1', 'Class 2']))

try:
    auc_test = roc_auc_score(y_test, y_test_prob, multi_class='ovr', average='weighted')
    print(f"AUC (weighted, OvR): {auc_test:.4f}")
except Exception as e:
    print(f"AUC calculation: {e}")

print("=" * 60)
print(f"FINAL MODEL PERFORMANCE SUMMARY")
print("=" * 60)
print(f"Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}")
print(f"Test Precision (weighted): {precision_score(y_test, y_test_pred, average='weighted'):.4f}")
print(f"Test Recall (weighted): {recall_score(y_test, y_test_pred, average='weighted'):.4f}")
print(f"Test F1-Score (weighted): {f1_score(y_test, y_test_pred, average='weighted'):.4f}")
print("=" * 60)
from sklearn.metrics import confusion_matrix, roc_curve, auc
from sklearn.preprocessing import label_binarize
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu', linewidths=0.5,
            xticklabels=['Class 0', 'Class 1', 'Class 2'],
            yticklabels=['Class 0', 'Class 1', 'Class 2'],
            cbar_kws={'label': 'Count'})
plt.title("Confusion Matrix - Final KNN Model (Wine Dataset)", fontsize=14, fontweight='bold')
plt.xlabel("Predicted Label", fontsize=12)
plt.ylabel("True Label", fontsize=12)
plt.tight_layout()
plt.show()


n_classes = 3
y_test_bin = label_binarize(y_test, classes=[0, 1, 2])

fpr = dict()
tpr = dict()
roc_auc = dict()

plt.figure(figsize=(10, 7))

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_test_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    plt.plot(fpr[i], tpr[i], linewidth=2,
             label=f'Class {i} (AUC = {roc_auc[i]:.3f})')

plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.500)')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curves - Final KNN Model (Wine Dataset - Multi-class OvR)', fontsize=14, fontweight='bold')
plt.legend(loc="lower right", fontsize=10)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("\n" + "=" * 60)
print("FINAL TEST SET EVALUATION METRICS")
print("=" * 60)
print(f"Test Accuracy:            {accuracy_score(y_test, y_test_pred):.4f}")
print(f"Test Precision (weighted): {precision_score(y_test, y_test_pred, average='weighted'):.4f}")
print(f"Test Recall (weighted):    {recall_score(y_test, y_test_pred, average='weighted'):.4f}")
print(f"Test F1-Score (weighted):  {f1_score(y_test, y_test_pred, average='weighted'):.4f}")

avg_auc = np.mean(list(roc_auc.values()))
print(f"Test AUC (macro average):  {avg_auc:.4f}")

print("\nPer-Class AUC Scores:")
for i in range(n_classes):
    print(f"  Class {i}: {roc_auc[i]:.4f}")

print("=" * 60)
from sklearn.decomposition import PCA
import numpy as np

print("Creating 2D Decision Boundary Visualization...")
print("=" * 60)

X_test_processed_for_plot = final_pipe.named_steps['scaler'].transform(
    pd.DataFrame(final_pipe.named_steps['imputer'].transform(X_test), columns=X_test.columns)
)

X_trainval_processed = final_pipe.named_steps['scaler'].transform(
    pd.DataFrame(final_pipe.named_steps['imputer'].transform(X_trainval), columns=X_trainval.columns)
)

pca = PCA(n_components=2, random_state=42)
pca.fit(X_trainval_processed)
X_test_pca = pca.transform(X_test_processed_for_plot)
X_trainval_pca = pca.transform(X_trainval_processed)

print(f"PCA Explained Variance Ratio: {pca.explained_variance_ratio_}")
print(f"Total Variance Explained: {sum(pca.explained_variance_ratio_):.2%}")

knn_vis = KNeighborsClassifier(
    n_neighbors=final_pipe.named_steps['knn'].n_neighbors,
    weights=final_pipe.named_steps['knn'].weights,
    metric=final_pipe.named_steps['knn'].metric,
    p=final_pipe.named_steps['knn'].p if hasattr(final_pipe.named_steps['knn'], 'p') else 2
)
knn_vis.fit(X_trainval_pca, y_trainval)

x_min, x_max = X_trainval_pca[:, 0].min() - 1, X_trainval_pca[:, 0].max() + 1
y_min, y_max = X_trainval_pca[:, 1].min() - 1, X_trainval_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))
grid = np.c_[xx.ravel(), yy.ravel()]
Z = knn_vis.predict(grid)
Z = Z.reshape(xx.shape)

plt.figure(figsize=(12, 8))

colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']  # Red, Teal, Blue for 3 classes
cmap = plt.matplotlib.colors.ListedColormap(colors)

plt.contourf(xx, yy, Z, alpha=0.4, cmap=cmap, levels=[-.5, 0.5, 1.5, 2.5])

scatter = plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1],
                     c=y_test, cmap=cmap,
                     edgecolor='black', s=100, linewidth=1.5,
                     label='Test Data')

cbar = plt.colorbar(scatter, ticks=[0, 1, 2])
cbar.set_label('Wine Class', fontsize=12)
cbar.ax.set_yticklabels(['Class 0', 'Class 1', 'Class 2'])

plt.title(f"2D Decision Boundary - KNN (k={knn_vis.n_neighbors}) on Wine Dataset\n" +
          f"PCA Projection: {sum(pca.explained_variance_ratio_):.1%} Variance Explained",
          fontsize=14, fontweight='bold')
plt.xlabel(f"Principal Component 1 ({pca.explained_variance_ratio_[0]:.1%} variance)", fontsize=12)
plt.ylabel(f"Principal Component 2 ({pca.explained_variance_ratio_[1]:.1%} variance)", fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("\nVisualization complete!")
print(f"k value used: {knn_vis.n_neighbors}")
print(f"Weight scheme: {knn_vis.weights}")
print(f"Distance metric: {knn_vis.metric}")
print("=" * 60)

import joblib

model_filename = "final_knn_wine_pipeline.pkl"
joblib.dump(final_pipe, model_filename)

print("=" * 60)
print("MODEL SAVED SUCCESSFULLY!")
print("=" * 60)
print(f"Filename: {model_filename}")
print(f"Location: Current working directory")
print("\nThis file contains:")
print("  ✓ Imputer (median strategy)")
print("  ✓ StandardScaler")
print("  ✓ KNN Classifier with optimized hyperparameters")
print("\nBest parameters from trained model:")
knn_model = final_pipe.named_steps['knn']
print(f"  • n_neighbors: {knn_model.n_neighbors}")
print(f"  • weights: {knn_model.weights}")
print(f"  • metric: {knn_model.metric}")
if hasattr(knn_model, 'p'):
    print(f"  • p: {knn_model.p}")
print("\n" + "=" * 60)
print("SUBMISSION CHECKLIST:")
print("=" * 60)
print("Upload to GitHub (https://github.com/mitra369/KNN_210112):")
print("  [ ] wine.csv (dataset)")
print("  [ ] your_notebook.ipynb (Google Colab notebook)")
print("  [ ] final_knn_wine_pipeline.pkl (trained model)")
print("  [ ] README.md (optional but recommended)")
print("\nMake sure your repository is PUBLIC for submission!")
print("=" * 60)
